---
title: "Clean data for fractal Analyses"
author: "Robin Sifre"
date: "`r Sys.Date()`"
output:
  rmdformats::readthedown:
    highlight: kate
    self_contained: true
---
# About
Cleans multi-level data & generates text on exclusions

## To-do
- For the cases where I added 'v01' to the calver data for DevSocAtt kids - verify that these were the first visits 
- Change the way you select which precision to use: "Audio-visual attention cues were presented at 3 time-points during each eye-tracking visit: at the beginning, interleaved with the Social and Pixelated movies, and at the end. Infants’ longest contiguous raw eye-tracking data for all available Attention Cue trials were analyzed for precision."

```{r constants1}
knitr::opts_chunk$set(echo = FALSE)

# User parameters
min_age = 0
max_age = 61
wdir = '~/Documents/Github/fractal-eye-analyses/'
cleaned_precision_dat = 0 # Are you using precision data that have already been cleaned?
```

# Where do you want to save data?

```{r constants2}
out_dir = paste(wdir, 'data/', sep = '')
file_name = paste(format(Sys.time(), "%Y %b %d"), 'cleaned_multilevel_data', sep = '-')
```

```{r}
out_dir
```

```{r load_libraries, message=FALSE, warning=FALSE, echo=FALSE}
library(tidyverse)
library(ggplot2)
library(knitr)
library(rmdformats)
library(rmarkdown)
library(DT)
library(lubridate)
library(DiagrammeR)
source('/Users/sifre002/Documents/GitHub/fractal-eye-analyses/analysis/Rfuncs/compareNA.R')
```

# Read data

```{r read_data, echo=TRUE, message=FALSE, warning=FALSE}
# List of ET sessions linked with DOB, Sex, etc. 
particList = read_csv(file = paste(wdir, 'data_audit/potential_etvis_in_loris.csv', sep=''))

# Concatenated output Output from pipeline in fractal-eye-analyses-MFDFA/
h <- read_csv(file = paste(wdir, 'data/h_out.txt', sep = ''))

# Concatenated Face-looking data 
aoi_data<- read.csv( paste(wdir, 'data/face_out.txt', sep ='') )

# Calibration precision spreadsheet  (from calver task & from DL task)
precision_info_calver <- read.csv( paste(wdir, 'data/calver/calver_trials/_output_reformatted.csv', sep=''), stringsAsFactors=FALSE)
precision_info_dl     <- read.csv( paste(wdir, 'data/calver/dl_trials/_output_reformatted.csv', sep=''), stringsAsFactors= FALSE)

# Log of cases that were skipped for some reason
log_calver = read.csv( paste(wdir, 'data/calver/calver_trials/_summary.txt', sep=''), stringsAsFactors=FALSE, header = FALSE)
log_dl = read.csv( paste(wdir, 'data/calver/dl_trials/_summary.txt', sep=''), stringsAsFactors=FALSE, header = FALSE)
```


```{r}
# Format vars
h=h %>%
  mutate(date=mdy(date)) %>%
  mutate(PSCID = paste(lapply(strsplit(id, '_'), '[', 1),
                       lapply(strsplit(id, '_'), '[', 2), sep= '_'))%>% 
  filter(!grepl('Bab', id, ignore.case=TRUE)) %>%
  rename(Hq_Neg5 = `Hq-5`, 
         Hq_Neg3 = `Hq-3`, 
         Hq_Neg1 = `Hq-1`,
         Hq_Pos1 = `Hq+1`,
         Hq_Pos3 = `Hq+3`,
         Hq_Pos5 = `Hq+5`)

# Keep cases with at least one visit 
particList = particList %>% filter(n_et>0)
```



Handle cases where data from the same visit was copied twice
```{r}
h = h %>%
  # Make variable that labels the id-movie-seg-longestFix-propInterp-date
  # If all of these are the same, then they are duplicates and should be removed 
  mutate(temp=paste(id,movie,seg, round(propInterp,3), date)) %>%
  group_by(temp) %>%
  mutate(n=c(1:n()))  %>%
  ungroup() %>%
  filter(n==1) %>%
  select(-n)

aoi_data = aoi_data %>%
  filter(id %in% h$id) %>%
  # Make variable that labels the id-movie-seg-longestFix-propInterp-date
  # If all of these are the same, then they are duplicates and should be removed 
  mutate(date = mdy(date)) %>%
  mutate(temp=paste(id,movie,seg, round(propInterpolated,3), date)) %>%
  group_by(temp) %>%
  mutate(n=n() )  %>%
  mutate(n=c(1:n()))  %>%
  ungroup() %>%
  filter(n==1) %>%
  select(-n)


```


```{r}
# Handle dups for face-looking
# dups = aoi_data %>% filter(n>1) %>% pull(temp) %>% unique()
# aoi_data$remove_dup=0
# for (i in dups) {
#   temp = aoi_data %>% filter(temp==i)
#   # If the date & longestFixDur match, assume this is a dup and remove the second ow
#   if (temp$date[1]==temp$date[2] & 
#       temp$longestFixDur[1]==temp$longestFixDur[2] & 
#       temp$faceCount[1]==temp$faceCount[2]) {
#     aoi_data[aoi_data$temp==temp$temp[1] & aoi_data$n==2,'remove_dup'] = 1
#   }
# }
# aoi_data = aoi_data %>% filter(remove_dup==0) %>% select(-c(remove_dup))

```


Check if missing data in particList
```{r particListcsv, echo=TRUE}
particList %>%
  filter(is.na(DOB) | is.na(Sex))%>% 
  select(CandID, PSCID, DOB, Sex, et_collected, et_age, et_date, cohort2)
```

# Format DFA output

Don't remove rows until the very end (unless they are not in `particList` or are missing their cohort info)

```{r echo=TRUE}
nrow(h)
```

```{r}
# Generate variables to help with analysis
h=h %>%
  relocate(PSCID, .after=id) %>% 
  # Create Dummies for CalVer & Pixelated
  mutate(CalVer=ifelse(grepl('Center|Left|Right', movie), 1, 0),
         Pix=ifelse(grepl('S', movie), 1, 0)) %>%
  mutate(use_h = ifelse(r2>=0.9, 1, 0))  %>%
  # Remove 'v' from id to merge with particList
  mutate(id = gsub('v', '', id))

```


```{r}
# Merge w/ participant info (CandID, DOB, Sex, n_et, cohort, cohort2, ageFirstVis)
temp = particList %>%
  select(PSCID, CandID,  DOB, Sex, n_et, cohort2, ageFirstVis) %>%
  distinct()

h= h %>%
  # Create variable for Participant ID (currently combined with session #)
  left_join(temp, by='PSCID') %>%
  # Variable to keep track of if theyre in participant list
  mutate(particList = ifelse(PSCID %in% particList$PSCID, 1, 0))

# Calculate age at visit
h = h %>%
  mutate(et_age = as.numeric(date-DOB) / 30.436875 )

```


Cases missing from particList. Remove for now 
```{r}
missing_info = h %>%
  filter(particList==0) %>%
  filter(!grepl('bab', id, ignore.case=TRUE)) %>%
  select(id, particList, cohort2) %>%
  unique()
missing_info
```

```{r}
h = h %>% filter(! id %in% missing_info$id)
```


Cases missing cohort (these were all covered above)
```{r}
h %>% 
  filter(particList==1) %>%
  filter(is.na(cohort2)) %>% 
  select(id,cohort2) %>% distinct()
```


## Bin the cohorts 
```{r}
h = h %>%
  mutate(cohort_bin = ifelse(grepl('dsa', cohort2), 'DevSocAtt', 'BcpBSLERP' ))
```


# Format CalVer output
```{r}
#Creates precisionInfo dataframe of average calibration XY precision information for each kid who made it through CalVer processing.
#Those with precision that is worse than threshold are coded as 999
# SKIP THIS PART IF YOU USING CALVER DATA THAT ALREADY HAVE BEEN CLEANED
if (cleaned_precision_dat==0) {
  source(paste(wdir, 'processing/clean_calver.R', sep = ''))
  calver=clean_calver(precision_info_calver)
  calver_thresh=calver[[1]]
  AvgPrec_calver=calver[[2]]
  
  calver_dl=clean_calver(precision_info_dl)
  calver_dl_thresh=calver_dl[[1]]
  AvgPrec_dl=calver_dl[[2]]
}
```

If visit does not have separate CalVer task, then use the trials in the DL File. I will use the threshold from the CalVer tasks for everyone (it is more stringent)
```{r echo=TRUE}
calver_thresh
calver_dl_thresh
```


```{r}
# Check if ID is missing session, or has multiple attempts
AvgPrec_calver = AvgPrec_calver %>%
  mutate(id_parts = unlist(lapply(strsplit(id,'_'), length) ),
         id = ifelse(id_parts==2, paste(id, '01',sep='_'), id),
         id = ifelse(id_parts>3, paste(sapply(strsplit(id,'_'), '[', 1),
                                       sapply(strsplit(id,'_'), '[', 2),
                                       sapply(strsplit(id,'_'), '[', 3), sep = '_'), id))


AvgPrec_dl = AvgPrec_dl %>%
  mutate(id_parts = unlist(lapply(strsplit(id,'_'), length) ),
         id = ifelse(id_parts==2, paste(id, '01',sep='_'), id),
         id = ifelse(id_parts>3, paste(sapply(strsplit(id,'_'), '[', 1),
                                       sapply(strsplit(id,'_'), '[', 2),
                                       sapply(strsplit(id,'_'), '[', 3), sep = '_'), id))


# If they have more than one value, use the better one 
AvgPrec_calver = AvgPrec_calver %>%
  select(-id_parts) %>%
  group_by(id) %>%
  mutate(AvgPrecision = max(AvgPrecision)) %>%
  ungroup() %>%
  distinct()

# If they have more than one value, use the better one 
AvgPrec_dl = AvgPrec_dl %>%
  select(-id_parts) %>%
  group_by(id) %>%
  mutate(AvgPrecision = max(AvgPrecision)) %>%
  ungroup()%>%
  distinct()
```

```{r}
# Check if the file was skipped (not enough data)
row = max(which(grepl('File', log_calver$V1)))
skipped1 = log_calver[row+1:nrow(log_calver), 'V1']
skipped1=skipped1[!is.na(skipped1)]
skipped1 = basename(skipped1)


row = max(which(grepl('File', log_dl$V1)))
skipped2 = log_dl[row+1:nrow(log_dl), 'V1']
skipped2 =skipped2[!is.na(skipped2)]
skipped2 = basename(skipped2)

# Format Ids from the tsv
skipped = c(skipped1,skipped2)
skipped = skipped[!grepl('distance|output', skipped)]
skipped = paste(sapply(strsplit(skipped, '_'), '[', 1),
      sapply(strsplit(skipped, '_'), '[', 2),
      sapply(strsplit(skipped, '_'), '[', 3), sep = '_')
skipped = gsub('v', '', skipped)
```



Determine which visits we need to pull calver from the DL task. 
```{r echo=TRUE}
dl_ids = AvgPrec_dl$id
cal_ids = AvgPrec_calver$id

h_ids = h %>% 
  pull(id) %>%
  unique()

no_cal = setdiff(h_ids, cal_ids)
```

Cases that don't have calver data, but also are not in the DL-calver output (could be bc not enough data to estimate the precision)
```{r}
missing = no_cal[no_cal %in% dl_ids==FALSE] 
truly_missing = missing[missing %in% particList$et_collected==FALSE]
```

There are `r length(missing)` visits with no precision data. `r length(truly_missing)` are NOT in `~/AUDIT_PASSED/`
```{r}
missing
```

Which missing cases were *not* skipped in the calver script? (Files are skipped if there is not enough fixation data on the attention-getter trials)
```{r}
missing[!missing %in% skipped]
```


Most of the cases with no separate calver file are from 2015 or earlier. 
```{r}
particList %>%
  select(CandID, et_date, et_collected, DL_flag, Cal_flag) %>%
  filter(et_collected %in% no_cal) %>%
  arrange(et_date) %>% 
  datatable()
```

```{r}
# Calver scores from the dancing ladies trial to keep and combine with calver scores from true calver
Avg_Prec = rbind(AvgPrec_dl %>% mutate(cal_source='DL'),
                 AvgPrec_calver %>% mutate(cal_source='Cal'))

# If they have preicsion data from calver, use that one instead of DL
Avg_Prec=Avg_Prec %>%
  group_by(id) %>%
  arrange(cal_source) %>%
  mutate(n=n()) %>%
  # If they have 2 and one of them is 999 remove that one,
  mutate(remove = ifelse(n==2 & AvgPrecision==999,1,0)) %>%
  filter(remove==0) %>%
  # If they still have 2 (they have 2 good ones), keep the one from calver
  mutate(remove = ifelse(n==2 & cal_source=='DL', 1,0)) %>%
  ungroup() %>%
  filter(remove==0) %>%
  select(-c(remove,n, cal_source))

dups = Avg_Prec %>% filter(duplicated(id)) %>% pull(id)
Avg_Prec %>% filter(id %in% dups)
```




```{r}
# Merge Precision data with H data 
h= h %>% left_join(Avg_Prec, by='id') 
```


# Flag time series for exclusion


```{r}
fussed_out = h %>%
  group_by(id) %>%
  # Calculate the number of trials
  mutate(n = n(),
         n_missing = sum(h==-9999)) %>%
  filter(n==n_missing) %>%
  pull(id) %>%
  unique()
```


Flag participants in `h_out.txt` who are a) not in the participant list, b) not in age-range, and c) fussed out

```{r echo=TRUE}
# Log who was removed 
h = h %>%
  mutate(remove = ifelse(particList==0, 1, 0),
         reason = ifelse(particList==0, 'Not in participant list', ''),
         remove = ifelse(is.na(cohort2), 1, remove),
         reason = ifelse(is.na(cohort2) & reason=='', 'Missing cohort', reason),
         remove = ifelse(et_age<min_age | et_age>max_age, 1, remove),
         reason = ifelse((et_age<min_age | et_age>max_age) & reason=='', 'Age range', reason),
         remove = ifelse(id %in% fussed_out, 1, remove),
         reason = ifelse(id %in% fussed_out & reason=='', 'Fussed out', reason)) %>% 
  mutate(reason = ifelse(is.na(reason), '', reason))
```


Flag participants in `h_out.txt` who are a) missing calver or b) have bad calver

```{r}
# If calver could not be calculated bc not enough data, or too poor data, count this as bad calver
h = h %>%
  mutate(remove = ifelse(id %in% skipped, 1, remove),
         reason = ifelse(id %in% skipped & reason =='', 'Bad Calver', reason))
```


```{r echo=TRUE}
# If there was enough data, but ultimately they were below the threshold (flagged as 999), count as BadCalver
h = h %>% 
  mutate(remove = ifelse(compareNA(AvgPrecision, 999), 1, remove),
         reason = ifelse(compareNA(AvgPrecision, 999) & reason=='', 'Bad Calver', reason),
         remove = ifelse(is.na(AvgPrecision), 1, remove),
         reason = ifelse(is.na(AvgPrecision) & reason=='', 'Bad Calver', reason)) 

```

If they fussed out, call it bad calver (couldn't calibrate)
```{r echo=TRUE}
h = h %>%
  mutate(reason = ifelse(grepl('Fuss', reason), 'Bad Calver', reason))
  
```

Check for Wonky calver

```{r}
h %>% 
  filter(AvgPrecision < 0) %>%
  select(id, AvgPrecision) %>%
  distinct()
```



Flag row in `h_out.txt` for high interpolation
```{r echo=TRUE}
interp_thresh = h %>%
  # Calculate 80th percentile of time series from visits that have good calibration 
  filter(compareNA(remove,0)) %>%
  pull(propInterp) %>%
  quantile(c(.8))
interp_thresh
```

For manuscript:
After visualizing the distribution, the 80th quantile threshold for the sample's proportion interpolated was calculated (`r round(interp_thresh[1], 3)`) and segments with a proportion exceeding that value were excluded from analyses

```{r echo=TRUE}
h = h %>%
  mutate(remove = ifelse(propInterp > interp_thresh[1], 1, remove),
         reason = ifelse(propInterp > interp_thresh[1] & reason=='', 'propInterp', reason))
```


Flag row in `h_out.txt` if the time series was too short (\<800 frames, which is roughly `r 800*3.33` ms)

```{r}
min_dur = 800*3.33

h = h %>%
  mutate(remove = ifelse(longestFixDur<min_dur, 1, remove),
         reason = ifelse(longestFixDur<min_dur & reason=='', 'Fixation too short', reason))

```

Flag row in `h_out.txt` if the the $r^2$ indicated that there was non-linear scaling function

```{r}
h = h %>%
  mutate(remove = ifelse(r2<0.9, 1, remove),
         reason = ifelse(r2<0.9 & reason=='', 'Bad R2', reason))
```

Reasons a row was removed

```{r}
h %>% filter(remove==1) %>% pull(reason) %>% unique()
```

# Flow on exclusion
Note - start with full sample (exclding cases with missing demographics). These were removed: 
```{r echo = TRUE}
h %>% filter(is.na(DOB)) %>% select(id, et_age) %>% arrange(id) %>% distinct()
h = h%>%filter(!is.na(DOB))
```

```{r}
removed = h %>% filter(remove==1)
h_clean = h %>% filter(remove==0)
```


```{r}
n_females = h %>% select(PSCID, Sex) %>% distinct() %>% pull(Sex) 
n_females = sum(grepl('female', n_females, ignore.case=TRUE))

# Summarise age 
age_summ = h %>% 
  select(id, et_age) %>% 
  distinct() %>% 
  summarize(mean_age = mean(et_age),
            min_age = min(et_age),
            max_age = max(et_age))
```

<b>Original Sample</b>.  `r nrow(h)` time-series of eye-tracking data were collected from <b>`r length(unique(h$PSCID))` `r round(age_summ$min_age, 2)`-`r round(age_summ$max_age, 2)`-month-old </b> infants (`r n_females` females, mean age= `r round(age_summ$mean_age,2)` months) across `r length(unique(h$id))` visits to the lab.  

```{r}
# How many visits per partipants 
vis = h %>%
  select(PSCID, id) %>%
  distinct() %>%
  group_by(PSCID) %>%
  summarize(n=n())
```

As part of a planned missingness design, children contributed between `r min(vis$n)` and `r max(vis$n)` waves of data across this age span (mean `r round(mean(vis$n),2)` waves) during visits to the lab. 


```{r}

# check_if_removed <- function(i, h){
#   removed_person_from_sample = 0
#   # Get this person's PSICD
#   p = h %>% filter(id==i) %>% pull(PSCID) %>% unique()
#   
#   # See if they have visit other than this one
#   temp = h %>% filter(PSCID==p, id!=i) %>% select(PSCID, id) %>% distinct()
#   if (nrow(temp) == 0) {
#     removed_person_from_sample=1
#   }
#   # See if th
#   return(removed_person_from_sample)
# }

```

## Age range
```{r tally_removed_age, cache=TRUE}
# Outside age-range
# # # # # # # # # 
survived = h %>%
  # Get list of Ids that are still there, after you remove for all the resons leading up to & including the current var
  filter(!grepl('Age', reason)) %>%
  select(id, PSCID,reason) %>%
  distinct()

removed_age = removed  %>%
  filter(reason=='Age range') %>% 
  select(PSCID, id, et_age) %>%
  distinct() %>%
  mutate(removed_PSCID_from_sample = ifelse(PSCID %in% survived$PSCID, 0, 1),
       removed_vis_from_sample = ifelse(id %in% survived$id, 0, 1))
```



## Bad Calver
```{r tally_removed_vislevel, cache=TRUE}
# Bad calver
# # # # # # # # # 
survived = h %>%
  # Get list of Ids that are still there, after you remove for all the resons leading up to & including propInterp
  filter(!grepl('Age|Cal', reason)) %>%
  select(id, PSCID,reason) %>%
  distinct()

removed_calver = removed %>%
  filter(grepl('Cal', reason)) %>% 
  select(PSCID, id) %>% 
  distinct() %>%
  mutate(removed_PSCID_from_sample = ifelse(PSCID %in% survived$PSCID, 0, 1))

PSCID_removed_calver = removed_calver %>% filter(removed_PSCID_from_sample==1) %>% pull(PSCID) %>% unique()
vis_removed_calver = removed_calver %>% pull(id) %>% unique()


```

<b>Quality-control exclusion criteria</b>. Infants’ eyes were calibrated to the eye-tracking equipment at the beginning of each visit, using the manufacturer’s five-point calibration procedure. Precision, or the distance between repeated samples of gaze points58, was included given that it is a measure of variability and thus is likely to influence DFA calculations that quantify nested patterns of variability. To measure the eye-tracker’s precision throughout the experiment, experimental trials were interleaved with audio-visual attention cues. Precision was included as a quality-control exclusion criterion due to its potential to influence fractal structure, given that it is an index of variation around a target stimulus. Audio-visual attention cues were presented at 3 time-points during each eye-tracking visit: at the beginning, interleaved with the Social and Pixelated movies, and at the end. Infants’ longest contiguous raw eye-tracking data for all available Attention Cue trials were analyzed for precision. Data from eye-tracking sessions with Root Mean Square Error of Approximation (RMSEA) values two standard deviations or more above the sample mean (indicating poor precision) were excluded. As such, data from eye-tracking sessions with an average RMSEA (averaged across X and Y axes for both eyes) greater than <b>`r round(calver_thresh,2)`</b> degrees of visual angle were excluded from analyses. 

As a result, data from <b>`r length(PSCID_removed_calver)` participants, `r length(vis_removed_calver)` eye-tracking visits, totaling `r removed %>% filter(grepl('Calver', reason)) %>% nrow()` </b> time-series were excluded from future analyses. For the remaining eye-tracking visits, the average precision was <b>`r mean(h_clean$AvgPrecision)`°</b> of visual angle.

## High interpolation levels

```{r tally_removed_interp, cache=TRUE}
# Flag time series w/ too much interpolated data
# # # # # # # # # # # # # # # # # # # # # # # # # 
survived = h %>%
  # Get list of Ids that are still there, after you remove for all the resons leading up to & including propInterp
  filter(!grepl('Age|Fuss|Calver|propInterp', reason)) %>%
  select(id, PSCID,reason) %>%
  distinct()
removed_interp = removed %>%
  filter(reason == 'propInterp') %>%
  select(PSCID,id, movie, seg) %>%
  mutate(removed_PSCID_from_sample = ifelse(PSCID %in% survived$PSCID, 0, 1),
         removed_vis_from_sample = ifelse(id %in% survived$id, 0, 1))


PSCID_removed_interp = removed_interp %>% 
  filter(removed_PSCID_from_sample==1) %>% pull(PSCID) %>% unique()

vis_removed_interp = removed_interp %>% 
  filter(removed_vis_from_sample==1)  %>% pull(id) %>% unique()

# Make sure these were not included in kids already removed
intersect(PSCID_removed_interp, c(PSCID_removed_calver))
intersect(vis_removed_interp, c(vis_removed_calver))
```

Blinks were identified using a noise-based algorithm. All data missing as a result of blinks (less than 200 ms)
were linearly interpolated using data from the last valid sample before the start of the blink, and the first valid sample after the end of the blink <b>(mean proportion interpolated = `r round(mean(h_clean$propInterp),2)`, range `r min(h_clean$propInterp)`–`r max(h_clean$propInterp)`). </b> \

```{r}
# Calculate mean % missing for the time series that have not yet been excluded 
temp = h %>% 
  filter(!grepl('Calver|participant list|propInterp', reason)) %>%
  pull(propMissing) %>%
  round(3)
```

After interpolation, infants had an average proportion of `r mean(temp)` (range `r min(temp)`–`r max(temp)`) residual missing data per segment. Given past work suggesting too much aggregation can bias detrended fluctuation analysis (e.g.,20), we chose a stringent quality control threshold for the permissible proportion interpolated for each segment. 

After visualizing the distribution, the 80th quantile threshold for the sample’s proportion interpolated was calculated (`r round(interp_thresh[1], 3)`) and segments with a proportion exceeding that value were excluded from analyses; data from an additional 
`r length(PSCID_removed_interp)` infants, 
`r length(vis_removed_interp)`  eye-tracking session, 
totaling `r removed %>% filter(grepl('Interp', reason)) %>% nrow()` time-series 
were removed from future analyses. 
The mean proportion of residual missing data for the remaining sample was `r mean(h_clean$propMissing)`. 
Proportions of interpolated and residual missing data were entered into all models as covariates. 

## Fixation too short

```{r tally_removed_tooshort, cache=TRUE}
# Flag time series w/ too short fix data
# # # # # # # # # # # # # # # # # # # # # # # # # 
survived = h %>%
  # Get list of Ids that are still there, after you remove for all the resons leading up to & including propInterp
  filter(!grepl('Age|Calver|propInterp|Fix', reason)) %>%
  select(id, PSCID,reason) %>%
  distinct()
removed_fix = removed %>%
  filter(reason=='Fixation too short') %>% 
  select(PSCID, id, movie, seg)  %>%
  mutate(removed_PSCID_from_sample = ifelse(PSCID %in% survived$PSCID, 0, 1),
         removed_vis_from_sample = ifelse(id %in% survived$id, 0, 1))


PSCID_removed_fix= removed_fix %>% 
  filter(removed_PSCID_from_sample==1) %>% pull(PSCID) %>% unique()

vis_removed_fix = removed_fix %>% 
  filter(removed_vis_from_sample==1)  %>% pull(id) %>% unique()

# Make sure these were not included in kids already removed
intersect(PSCID_removed_fix, c(PSCID_removed_calver,PSCID_removed_interp))
intersect(vis_removed_fix, c(vis_removed_calver,vis_removed_interp))
```

Based on ----, we removed time series with fewer than 800 data points. Accordingly, data from an additional 
`r length(PSCID_removed_fix)` infants, 
`r length(vis_removed_fix)` eye-tracking visits, 
totaling to `r removed %>% filter(grepl('Fix', reason)) %>% nrow()` time series were excluded from future analyses


## Bad R2
```{r tally_removed_r2, cache=TRUE}
# Flag time series w/ Bad R2
# # # # # # # # # # # # # # # # # # # # # # # # # 
survived = h %>%
  # Get list of Ids that are still there, after you remove for all the resons leading up to & including propInterp
  filter(!grepl('Age|Calver|propInterp|Fix|R2', reason)) %>%
  select(id, PSCID,reason) %>%
  distinct()

removed_r2 = removed %>%
  filter(reason=='Bad R2') %>% 
  select(PSCID, id, movie, seg)  %>%
  mutate(removed_PSCID_from_sample = ifelse(PSCID %in% survived$PSCID, 0, 1),
         removed_vis_from_sample = ifelse(id %in% survived$id, 0, 1))


PSCID_removed_r2= removed_r2 %>% 
  filter(removed_PSCID_from_sample==1) %>% pull(PSCID) %>% unique()

vis_removed_r2 = removed_r2 %>% 
  filter(removed_vis_from_sample==1)  %>% pull(id) %>% unique()

# Make sure these were not included in kids already removed
intersect(PSCID_removed_r2, 
          c(PSCID_removed_calver,PSCID_removed_interp,PSCID_removed_fix))

intersect(vis_removed_r2, 
          c(vis_removed_calver,vis_removed_interp, vis_removed_fix))
```



Finally, .... data from an additional 
`r length(PSCID_removed_r2)` infants, 
`r length(vis_removed_r2)` eye-tracking visits, 
totaling to `r removed %>% filter(grepl('R2', reason)) %>% nrow()` time series were excluded from future analyses

```{r}
# Tallies for final vis
temp = h_clean %>%
  select(PSCID,id, Sex) %>%
  distinct() %>%
  group_by(PSCID) %>%
  mutate(n_vis= n()) %>%
  ungroup() %>%
  select(-id) %>%
  distinct()

final_n = nrow(temp)
final_n_female = sum(temp$Sex=='Female')
final_n_vis = sum(temp$n_vis)
final_n_timeseries = nrow(h_clean)
```


# Flow chart of exclusion
```{r}
# For segment-level variables, need to make data.frame at visit-level 
# (otherwise inflating missing tallies)
#interp = removed_interp %>%  select(-c(movie,seg)) %>% distinct() 
#tooshort = removed_fix  %>% select(-c(movie,seg)) %>% distinct() 
#r2 = removed_r2 %>% select(-c(movie,seg)) %>% distinct() 


lab1 = paste('Original Sample \n ', 
       'Individuals = ', length(unique(h$PSCID)), ' (', n_females, ' females) \n',
      'Visits = ', length(unique(h$id)), '\n',
      'Time-series = ', nrow(h), sep='')

lab_cal = paste('Excluded for low eye-tracker precision \n', 
       'Individuals = ', length(PSCID_removed_calver), '\n',
      'Visits = ', length(vis_removed_calver), '\n',
      'Time-series = ', removed %>% filter(grepl('Cal', reason)) %>% nrow(), sep='')

lab_interp = paste('Excluded for high interpolation levels \n', 
       'Individuals = ', length(PSCID_removed_interp), '\n',
      'Visits = ', length(vis_removed_interp), '\n',
      'Time-series = ', removed %>% filter(grepl('Interp', reason)) %>% nrow(), sep='')

lab_tooshort = paste('Excluded for time series < 800 \n', 
       'Individuals = ', length(PSCID_removed_fix), '\n',
      'Visits = ', length(vis_removed_fix), '\n',
      'Time-series = ', removed %>% filter(grepl('Fix', reason)) %>% nrow(), sep='')

lab_r2 =  paste('Excluded for poor linear fit \n', 
      'Individuals = ', length(PSCID_removed_r2), '\n',
      'Visits = ', length(vis_removed_r2), '\n',
      'Time-series = ', removed %>% filter(grepl('R2', reason)) %>% nrow(), sep='')

lab_final = paste('Final Sample \n',
                  'Individuals = ', final_n, ' (', final_n_female, ' females) \n',
                  'Visits = ', final_n_vis, '\n',
                  'Time-series = ', final_n_timeseries, sep ='')

grViz("digraph flowchart {
      # node definitions with substituted label text
      node [fontname = Helvetica, shape = rectangle]        
      tab2 [label = '@@2-1']
      tab3 [label = '@@3-1']
      tab4 [label = '@@4-1']
      tab5 [label = '@@5-1']

      node [fontname = Helvetica, shape = ellipse]
      tab1 [label = '@@1-1']
      tab6 [label = '@@6-1']
       # edge definitions with the node IDs
      tab1 -> tab2 -> tab3 -> tab4 -> tab5 -> tab6;
      }

      [1]: lab1
      [2]: lab_cal
      [3]: lab_interp
      [4]: lab_tooshort
      [5]: lab_r2
      [6]: lab_final

      
      ")
```




# Clean AOI data 

```{r clean_aoi, echo=TRUE}
aoi_data2 = aoi_data %>%
  rename(longestFixFace=longestFixDur) %>%
  mutate(id=as.character(id),
         id = gsub('v', '', id),
         movie=as.character(movie)) %>%
  mutate(propFace_seg = faceCount / (faceCount + otherCount))

# Cases where the data are all missing
aoi_data2 = aoi_data2 %>% 
  filter(!is.na(propFace_seg)) 
```

Are any visits in `h_clean` that are not in `aoi_data2`?
```{r}
setdiff(h_clean$id, aoi_data2$id)

```


```{r}
# Merge h and aoi_data2 by temp (string that contains unique identifier for each time series)
h2=left_join(h_clean, 
             aoi_data2 %>% select(temp, faceCount,otherCount,propFace_seg), by='temp')

```

 Merged data
```{r}
h2 %>% 
  select(id, PSCID, movie, seg, date, longestFixDur, 
         h, r2, remove, faceCount, otherCount, propFace_seg)  %>%
  head(20)
```


# Create person-, visit-, and movie-centered variables. These are used to parse variance for modelling.

## Grand means

```{r echo = TRUE}
# Get grand mean for average age
AveAge = h2 %>%
  # Pull age for each person's visit (average age calculated so that their age at each visit is weighted online once)
  group_by(id)%>%
  summarise(Age = unique(et_age)) %>%
  # Calculate sample's average Age 
  summarise(Age = mean(Age) ) %>%
  ungroup()

# Get grand mean for average longest fix
age_grand_mean = mean(AveAge$Age)
aveLongFix=mean(h2$longestFixDur)
avePrec=mean(h2$AvgPrecision)
avePropFace = mean(h2$propFace_seg, na.rm = TRUE)
avePropFace = mean(aoi_data2$propFace_seg)

# Create varaible for person's age centered at grand mean 
h2=h2%>%
  mutate(Age_centeredGrandmean=as.numeric(et_age-age_grand_mean)) 
```

-   Average age = `r age_grand_mean`
-   Average longest fix duration = `r aveLongFix`
-   Average calver precision = `r avePrec`
-   Average % face-looking = `r avePropFace`

## Person-level variables

```{r echo = TRUE}
# Creating level 3 (person) variables:
# Average age: Participant's average age (weight each visit once)
# Average Interpolated: mean prop interp
# Average precision: mean precision (weight each visit once)
# Average longest fix: mean longest fix, centered on grand mean
personLevel_var=h2%>%
  group_by(PSCID)%>%
  summarise(AveAge_person=mean(Age_centeredGrandmean),
            AveInterp_person=mean(propInterp),
            AveMissing_person=mean(propMissing),
            AvePrecision_person = mean(AvgPrecision),
            AveLongestFix_person=mean(longestFixDur),
            AvePropFace_person = sum(faceCount, na.rm = TRUE) / ( sum(faceCount, na.rm = TRUE) + sum(otherCount, na.rm = TRUE) ),
            n_person=n())


```

# Session-level variables

```{r echo=TRUE}
# Session age = Age_centeredGrandmean (no within-session variability)
# Average interpolated: Mean prop interp, for the session
# Average Missing: Mean prop Missing
# Average longest fix: mean longest fix, centered on grand mean
# Average face looking: mean face looking for the session
sessionLevel_var = h2 %>%
  group_by(id) %>%
  summarise(AveInterp_session = mean(propInterp),
            AveMissing_session = mean(propMissing),
            AveLongestFix_session = mean(longestFixDur),
            AvePropFace_session = sum(faceCount, na.rm = TRUE) / 
              ( sum(faceCount, na.rm = TRUE) + sum(otherCount, na.rm = TRUE) ))

```

# Movie-level

```{r echo=TRUE}
movieLevel_var = h2 %>%
  group_by(PSCID, id, movie) %>%
  summarise(AveInterp_movie = mean(propInterp),
            AveMissing_movie = mean(propMissing),
            AveLongestFix_movie = mean(longestFixDur),
            AvePropFace_movie = sum(faceCount, na.rm = TRUE) / 
              (sum(faceCount, na.rm = TRUE) + sum(otherCount, na.rm = TRUE)) )
```

```{r}
# # # # # # # # # # # #
# Create centered variables 
# # # # # # # # # # # #

# Merge person-level
h3 = merge(h2, personLevel_var, by = 'PSCID')
# Merge visit-level
h4 = merge(h3, sessionLevel_var, by = 'id')
# Merge movie-level
h5 = merge(h4, movieLevel_var, by = c('PSCID', 'id', 'movie'))

# Create PERSON_LEVEL variables centered at GRAND MEAN
h5$propInterp_person_grandmeancentered = h5$AveInterp_person - mean(h5$propInterp)
h5$longestFix_person_grandmeancentered  = h5$AveLongestFix_person - mean(h5$longestFixDur)
h5$propFace_person_grandmeancentered  = h5$AvePropFace_person - mean(h5$propFace_seg, na.rm = TRUE)
h5$precision_person_grandmeancentered  = h5$AvePrecision_person - mean(h5$AvgPrecision, na.rm=TRUE)
h5$age_centered_grandmean = h5$et_age - AveAge$Age

# Create VISIT-LEVEL variables centered at PERSON-LEVEL
h5$propInterp_session_groupmeancentered = h5$AveInterp_session - h5$AveInterp_person
h5$longestFix_session_groupmeancentered = h5$AveLongestFix_session - h5$AveLongestFix_person
h5$propFace_session_groupmeancentered = h5$AvePropFace_session - h5$AvePropFace_person
h5$precision_session_groupmeancentered = h5$AvgPrecision - h5$AvePrecision_person # each visit has one Precision_RMS_X_Y

# Create MOVIE-LEVEL VARIABLES centered at VISIT-LEVEL
h5$propInterp_movie_groupmeancentered = h5$AveInterp_movie - h5$AveInterp_session
h5$longestFix_movie_groupmeancentered = h5$AveLongestFix_movie - h5$AveLongestFix_session
h5$propFace_movie_groupmeancentered = h5$AvePropFace_movie - h5$AvePropFace_session

# Create SEGMENT-LEVEL VARIABLES centered at MOVIE LEVEL
h5$propInterp_segment_groupmeancentered = h5$propInterp - h5$AveInterp_movie
h5$longestFix_segment_groupmeancentered = h5$longestFixDur - h5$AveLongestFix_movie
h5$propFace_segment_groupmeancentered = h5$propFace_seg - h5$AvePropFace_movie


# Code for error checking 
# test = h5 %>% filter(id == 'JE000053_03_01' & movie == '01_converted.avi')
# test$AveAge_person
# 
# 
# test = h5 %>% filter(id == 'JE000053_03_01') %>%
#   dplyr::select(id,movie,seg,
#                 AvePropFace_person, AvePropFace_session, AvePropFace_movie,
#                 faceCount,otherCount,propFace_seg, propFace_segment_groupmeancentered, 
#                 propFace_movie_groupmeancentered, propFace_session_groupmeancentered,propFace_person_grandmeancentered)
# 
# test = h5 %>% filter(id == 'JE000053_03_01') %>%
#   dplyr::select(id, movie, seg, 
#                 AvePropFace_person, AvePropFace_session, AvePropFace_movie,
#                 faceCount,otherCount,propFace_seg, propFace_segment_groupmeancentered, 
#                 propFace_movie_groupmeancentered, propFace_session_groupmeancentered,propFace_person_grandmeancentered)
# # test visit-level face avg 
# sum(test$faceCount, na.rm = TRUE) / (sum(test$faceCount, na.rm = TRUE)  + sum(test$otherCount, na.rm = TRUE))
# # Test movie-level face 
# a = test %>% filter(movie == '01_converted.avi') 
# sum(a$faceCount, na.rm = TRUE) / (sum(a$faceCount, na.rm = TRUE)  + sum(a$otherCount, na.rm = TRUE))
# # test - segment level prop face
# test[1, 'faceCount'] / (test[1, 'faceCount'] + test[1, 'otherCount'])
# test[1, 'propFace_seg']
# # test segment - level cnetering
# test[1, 'propFace_seg'] - test[1, 'AvePropFace_movie'] == test[1, 'propFace_segment_groupmeancentered']
# test[1, 'AvePropFace_movie'] - test[1, 'AvePropFace_session'] == test[1, 'propFace_movie_groupmeancentered']
# test[1, 'AvePropFace_session'] - test[1, 'AvePropFace_person'] == test[1, 'propFace_session_groupmeancentered']
# test[1, 'AvePropFace_person'] - avePropFace == test[1, 'propFace_person_grandmeancentered']


```

# Final tally of cases before any exlcusionary criteria are applied 
```{r}
# Count of individuals from each cohort
h5 %>% 
  select(PSCID, cohort2) %>%
  distinct() %>%
  count(cohort2)

# Count of visits from each cohort
h5 %>% 
  select(id, cohort2) %>%
  distinct() %>%
  count(cohort2)

h5 %>%
  group_by(cohort_bin) %>%
  summarise(mean_vis_count = mean(n_et),
            min = min(n_et),
            max = max(n_et))

h5 %>%
  group_by(cohort2) %>%
  summarise(mean_vis_count = mean(n_et),
            min = min(n_et),
            max = max(n_et))

```

Summary of age 
```{r}
summary(h5$et_age)
```

```{r}
h5 %>%
  group_by(cohort_bin) %>%
  summarise(mean_et_age = mean(et_age),
            min = min(n_et),
            max = max(n_et))
```


```{r cleandat_longit_sample, cache=TRUE}
ggplot(data = h5, aes(y = reorder(CandID, ageFirstVis), x = round(et_age,0))) +
  geom_line(color = 'grey') +
  geom_point(aes(color=cohort_bin), size=1.1) +
  scale_color_manual(values = c('springgreen4', 'red'), labels=c('Cohort 1', 'Cohort 2')) +
  scale_x_continuous(breaks = c(3, 4, 6, 7, 9, 10, 12, 13, 24, 30, 36))+
  scale_y_discrete()+
  theme_bw() + 
  theme(axis.text.y = element_blank(),
        axis.text.x = element_text(size=12),
        axis.title = element_text(size=14, face='bold')) +
  labs(x = 'ET Age (months)', y = 'Individual', colour='Cohort') 
```



# Save clean data w/ variables generated 
```{r echo=TRUE}
today = format(Sys.Date(), format="%Y-%m-%d")
write.csv(h5, paste(out_dir, today, '-h_clean.csv', sep = ''))
write.csv(removed, paste(out_dir, today, '-h_removed.csv', sep = ''))

```

