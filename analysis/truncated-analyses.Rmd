---
title: "DFA Simulation"
date: "`r Sys.Date()`"
output:
  rmdformats::readthedown:
    highlight: kate
    self_contained: true
---


```{r setup, echo=FALSE, warning=FALSE,message=FALSE}
library(knitr)
library(rmdformats)
library(rmarkdown)
library(tidyverse)
library(corrplot)
library(reshape2)
library(lme4)
library(MuMIn)
library(sjstats)
library(AICcmodavg)
library(png)
library(DT)
library(bbmle)

## Global options
options(max.print="75")
opts_chunk$set(echo=FALSE,
	             cache=FALSE,
               prompt=FALSE,
               tidy=TRUE,
               comment=NA,
               message=FALSE,
               warning=FALSE,
               include=TRUE,
               cache.extra = tools::md5sum(
                 c('/Users/sifre002/Box/sifre002/7_MatFiles/01_Complexity/stability-experiments/truncated_ts/constant_segment_length/stability_out.txt',
                 '/Users/sifre002/Documents/Code/fractal-eye-analyses/out/stability_analyses/simulated_series/simulated_stability_out_10000.txt')))

opts_knit$set(width=75)
options(scipen=999)
# Control # of digits in output
knitr::knit_hooks$set(inline = function(x){
  x <- sprintf('%1.3f',x)
  paste(x, collapse=', ')
})


```




# About  

## Background
I use eye-tracking data to measure attention development in babies. One metric I use is <b>fractality</b> as a way to estimate the organizational structure of variance in a time series - how predictable/random is the noise?  

## The challenge
To mesaure how fractal a time series is, I use an analysis called <b>Detrended Fluctuation Analysis (DFA)</b>.  
The recommended minimum time series length for DFA on "biomedical time series data" is 1,000 contiguous samples.  Because of how noisy infant eye-tracking data is, we exclude a lot of data that does not meet this minimum requirement.  
  
<b>The goal of these analyses is to determine the minimum required time series length for DFA using 300 Hz eye-tracking data.</b> Can we include shorter time series so that we can include more data in future analyses?   

## The approach
DFA provides a summary output for a given time series: $h$  

If we take time series of data, and chop them up into smaller and smaller pieces, how small can we go before:  
1) the segment length starts to predict our $h$ estimates, and   
2) our $h$ estimates become unstable?  

For each estimate of $h$, I also calculated an $r^$ estimate of model fit for a linear relationship between power and frequency. I will also examine how segment length impacts $r^$ values.  

### Methods notes 
- All time series passed QA/QC. I selected the settings for calculating $H$ (e.g. scmin, scmax) based on prior stability analyses (`scmin=8`, `scres=8`, and `scmaxDiv=4`).  
- Time series were segmented from the beginning & from the end. Truncated segment lengths start from 100 and increase by 100.  
- For example, if a time series had 1,000 samples, the time series from the <b>first index</b> were ([1:100], [1:200], ... [1:1000]), and the time series from the <b>last index</b> were ([900:1000], [800:1000], ... [100:1000]).  
- This gives us 2 100-length segments (first 100 samples, last 100 samples), 2 200-length segments (first 200 samples, last 200 samples), and so on.  
- The last few frames of the time series were trimmed so that time series was divisible by 100.  
- Longer time series have more segments. $n segments=(Length / 100 *2) -1$


```{r example_fig, cache=TRUE}
frombeg_start=rep(1,10)
frombeg_end=seq(100,1000,100) # 100:1000

plot_dat_beg=data.frame(x_start=rep(1,10), x_end=seq(100,1000,100), type='Start From First Sample')
plot_dat_end=data.frame(x_start=seq(100,900,100), x_end=rep(1000,9), type='End at Last Sample')

plot_dat=rbind(plot_dat_beg, plot_dat_end)
plot_dat$y = c(1:10, 1.5:9.5) # ordered from type
plot_dat$type = as.character(plot_dat$type)
plot_dat[10, 'type']='Complete'

ggplot(data=plot_dat, aes(color=type)) + 
  geom_segment(aes(x=x_start,xend=x_end, y=y, yend=y)) +
  scale_x_continuous(breaks=seq(0,1000,100))  +
  scale_y_continuous(labels = c('length=900','length=900'), breaks = c(1.5,9)) + 
  theme_bw() + 
  labs(title='How truncated segments were generated from a time series', 
       x='Example time series, length=1,020', y='',
       caption='Note that last 20 frames are truncated so that the length is divisible by 100 \n to allow for equal segmant sizes across all time series.')

```

*Example of truncated segments*
```{r ex_fig_beg, echo=FALSE, fig.fullwidth=TRUE, cache=TRUE}
img <- readPNG("/Users/sifre002/Documents/Writing/Dissertation/E1/figs/Trunc_Beg.png")
grid::grid.raster(img)
```

```{r ex_fig_end, echo=FALSE, fig.fullwidth=TRUE, cache=TRUE}
img <- readPNG("/Users/sifre002/Documents/Writing/Dissertation/E1/figs/Trunc_End.png")
grid::grid.raster(img)
```

```{r}
# keep track of model results for final table
int_out = c(); lin_out = c(); quad_out = c(); model_out = c();
```


# Real data
## Make sure all time series in (2020) paper is included in the present analyses 
```{r, cache.extra=TRUE, echo = TRUE}
# Read stability analysis data 
dat=read.csv('/Users/sifre002/Documents/Code/fractal-eye-analyses/out/stability_analyses/constant_segment_length/stability_out.txt', stringsAsFactors = FALSE)
# Check that data match ms data
ms = read.csv('/Users/sifre002/Documents/Code/fractal-eye-analyses/data/cleaned_data.csv')
# Read stability data from simuilated time series 
simdat=read.csv('/Users/sifre002/Documents/Code/fractal-eye-analyses/out/stability_analyses/simulated_series/simulated_stability_out_10000.txt', stringsAsFactors = FALSE)
```
- `stability_out.txt`: File with \u03b1 and $r^2$ estimated for each truncated segment.  
- `cleaned_data.csv`: File with all time series included in the final (2020) manuscript (the sample to be included in these analyses).  

```{r datacheck, echo=TRUE}
# List of time series that were included in the 2020 ms 
ms =ms %>%
  dplyr::select(id, movie, seg, h,
              remove_age, remove_precision, remove_propInterp,remove_tooshort) %>%
  # Include time series that passed QC
  filter(remove_age ==0, 
         remove_precision==0,
         remove_propInterp==0, remove_tooshort == 0) %>%
  # Generate a distinct list of time series (data are long, want one row per time series)
  distinct() %>%
  dplyr::select(id,movie,seg,h) %>%
  mutate(movie=gsub('.avi','',movie),
         movie_seg=paste(movie,seg,sep='_'),
         uid=paste(id, movie_seg)) 
ms_ids = ms%>%dplyr::select(uid) %>%distinct()
dat=dat %>% mutate(uid = paste(id, movie_seg))

```

<b><u>List of participant in the manuscript not in the current analysis</u>:</b>
```{r, include=TRUE}
# Who is in the MS that is not in stability analyses?
missing_ts = setdiff(ms_ids$uid, dat$uid) 
missing_ids = lapply(strsplit(missing_ts, ' '), '[[', 1)
unlist(missing_ids) %>% unique()  
```

```{r, include=TRUE}
# Filter for time series that were included in the manuscript 
dat2 = dat %>%
  filter(uid %in% ms$uid)
# Double check that all of the time series are included here 
if (!length(unique(dat2$uid)) == nrow(ms)){
  print('Warning: Time series does not match 2020 publication')
}
```

<b><u>After retaining only the time series from the manuscript</u>:</b>  
```{r}
# Info on time series length of the original data 
n_trunc=dat2 %>%
  dplyr::select(uid, trunc_end) %>%
  group_by(uid) %>%
  dplyr::summarize(orig_len = max(trunc_end)) %>%
  ungroup() %>%
  mutate(n=(orig_len)/100) %>%
  mutate(n_series = (n*2)-1) %>%
  dplyr::select(-c(n))

nrow(dat2)
sum(n_trunc$n_series)
```
- `r sum(n_trunc$n_series)` truncated segments were generated.  
- The average rounded length of the original times series was `r mean(n_trunc$orig_len)` (min=`r min(n_trunc$orig_len)`, max=`r max(n_trunc$orig_len)`).  
- Each time series of length *N* generated [(*N*/100)*2]-1 truncated time series.  The average number of truncated time series generated per original series was `r as.integer(mean(n_trunc$n_series))` (min=`r as.integer(min(n_trunc$n_series))`, max=`r as.integer(max(n_trunc$n_series))`).  
- Of the original time series, `r as.integer(sum(n_trunc$orig_len>1000))` were longer than 1,000 samples.


```{r clean_data, echo = TRUE}
x=2
# Make Variables to help with analyses
dat2 = dat2 %>%
  # Dummy variables for trial type 
  mutate(calver = ifelse(grepl('Center|Left|Right',movie_seg), 1, 0),
         pix = ifelse(grepl('S', movie_seg), 1, 0),
         dl = ifelse(calver==0&pix==0, 1, 0)) %>%
  # Factor of stimulus type
  mutate(trial=ifelse(calver==1, 'CalVer', 'other'),
         trial=ifelse(pix==1, 'Pixelated', trial),
         trial=ifelse(dl==1, 'Social', trial)) %>%
  # Dummy var indicating MS setting
  mutate(ms_settings = ifelse(scmaxdiv==4 & scmin==4 & scres==4, 1, 0))  %>%
  # Participant ID 
  mutate(Participant=paste(lapply(strsplit(id,'_'), '[', 1), lapply(strsplit(id,'_'), '[', 2), sep='_')) %>%
  # Unique identifier for each segment
  mutate(longname=paste(id, movie_seg, sep='_')) %>%
  # Length of time series
  mutate(trunc_length=trunc_end-trunc_start,
         trunc_length_bin=round(trunc_length, -2), # Rounding to a neg number means rounding to power of 10
         trunc_length_bin=trunc_length_bin/100) %>%  # Easier for figs 
  # Flag the original length t.s. 
  # Create variable that indicates whether truncating from the beginning or end
  mutate(trunc_version=ifelse(trunc_start==1, 'beg','end')) %>%
  mutate(r2bin = ifelse(r2>=0.9,1,0))
```


```{r makewide, cache=TRUE}
dfa=dat2 %>% 
  dplyr::select(Participant, id, movie_seg, longname, h, r2,trial, trunc_length, trunc_length_bin, trunc_version)
#Create wide data
dfa.wide.beg=dcast(dfa %>%
                   filter(trunc_version=='beg') %>%
                         # trunc_length_bin<=15) %>% 
                   dplyr::select(longname, h, trunc_length_bin),
                   longname ~ trunc_length_bin,
                   value.var='h')

dfa.wide.end=dcast(dfa %>%
                   filter(trunc_version=='end') %>%
                          #trunc_length_bin<=15) %>% 
                   dplyr::select(longname, h, trunc_length_bin),
                   longname ~ trunc_length_bin,
                   value.var='h')
```


## How many segments were made for each time series?  
```{r barplot_nsegs, cache=TRUE, echo = TRUE}
###########################################
# Missing data patterns
###########################################
# Recode as 1=has data, 0=no data 
miss=dfa.wide.beg %>% dplyr::select(-c('longname'))
miss[is.na(miss)]=0
miss[miss!=0]=1
mean_dat=round(colMeans(miss)*100,0)

data.frame(pct_ts = mean_dat, ts_length = seq(100,2900,100)) %>%
  ggplot(data=., aes(x=ts_length, y=pct_ts)) + 
  geom_bar(stat = 'identity') + 
  geom_hline(yintercept = 25, color = 'red') +
  scale_x_continuous(breaks=seq(100,2800,300)) + 
  labs(x='Truncated Segment Length', y = 'Proportion of Time Series Represented') +
  theme_bw() + 
  theme(text = element_text(size=16))
```

```{r echo=TRUE, include = TRUE}
dfa2 = dat2 %>% 
  filter(trunc_length_bin<=22)
n_trunc2=dfa2 %>%
  dplyr::select(uid, trunc_length) %>%
  group_by(uid) %>%
  dplyr::summarize(n = n()) 
```
After removing truncated time series >=2300, the final sample included `r as.integer(nrow(dfa2))`.

```{r}
summary(dfa2$h)
```


```{r}
nrow(dfa2)
sum(n_trunc2$n) 
nrow(n_trunc2)
```

```{r echo=TRUE}
summary(n_trunc2$n) # Number of segmented versions per time series
```

# Impact on *R2*  -  real data

## Preliminary Visualizations 
The figures for $r^2$ values tell a different story. $r^2$ is the fit between a linear slope between log(power) and log(frequency), and the actual RMS values estimated at different scales. A low $r^2$ values suggests that there may not actually be power-law scaling.  
* Should be noted that the median $r^2$ value is high at each segment length.  
* Even for time series with 1,000 samples, a small % have low $r^2$ values. Perhaps we should exclude time series with low $r^2$ values?
```{r figs_r2}
ggplot(data=dfa2, aes(x=trunc_length_bin, y=r2)) +
  geom_boxplot(aes(group=trunc_length_bin, alpha=.2)) + 
  scale_x_continuous(breaks=unique(dfa2$trunc_length_bin)) +
  theme_bw() +
  labs(x='Length of time series segment (1 unit=100 samples)', y=expression(r^2))
```

Skew of $r^2$
```{r echo=TRUE}
quantile(dfa2$r2, seq(.05,.9,.1))
sum(dfa2$r2>=0.9)/nrow(dfa2)
```
- $r^2$ is high for almost all of the data.  
- Because of the skew, I will define <b>good model fit</b> as $r^2$ values >=0.9. Logistic regression models to determine how segment length impacts the probability of good model fit.  

## Visualize % of good model fit time series for each segment length
```{r r2_barplot_realdat}
prop.good = dfa2 %>%
  group_by(trunc_length_bin) %>%
  summarize(n_series = n(),
            n_good = sum(r2bin==1)) %>%
  ungroup() %>%
  mutate(prop_good = n_good/n_series,
         prop_good_c = prop_good - 0.5)

ggplot(data=prop.good, aes(x=trunc_length_bin, y = prop_good_c)) + 
  geom_col() +
  coord_flip() + 
  scale_x_continuous(breaks = seq(1,22,2),
                     labels = seq(100,2200, 200)) +
  scale_y_continuous(breaks = seq(0.0, 0.5, 0.1),
                     labels = seq(0.5, 1.0, 0.1)) + 
  labs(y= expression(paste('Proportion of time series with ', R^2, ' >= 0.9')), x = 'Length of truncated series') +
  theme_bw() +
  theme(axis.text = element_text(size=14),
        text = element_text(size=16))
```

```{r}
prop.good %>%
  select(trunc_length_bin, prop_good) %>%
  mutate(prop_good = round(prop.good$prop_good,3)*100)
```


# Impact on mean *h* estimates - real data 

## Preliminary Visualizations
<u>Does <b>a) the direction of truncating</b> or <b>b)the trial type</b> seem to moderate the relationship between segment length and $h$?</u> 

The range of $H$ estimates gest tighter the longer the segment. No clear impact of Trial type (calver, pix, or social) or whether segment was truncated from the beginning or end of the time series. 
```{r length_vs_H_begend}
# Does truncating from beginning/end matter? 
ggplot(data=dfa2, aes(x=trunc_length_bin, y=h, color=trunc_version)) + 
  geom_point(alpha=.2) + 
  theme_bw() + 
  labs(x='Length of time series segment (1 unit=100 samples)', y='\u03b1') + 
  scale_x_continuous(breaks=unique(dfa2$trunc_length_bin)) 
```


Boxplots of $H$ to show interquartile range. 
```{r h_boxplot}
ggplot(data=dfa2, aes(x=trunc_length_bin, y=h)) +
  geom_boxplot(aes(group=trunc_length_bin, alpha=.2)) + 
  scale_x_continuous(breaks=unique(dfa2$trunc_length_bin)) +
  scale_y_continuous(limits = c(-1.07, 2.25)) + # Scale to match visualizations for real data 
  theme(legend.position = 'none', panel.background = element_rect(fill='white', color = 'black')) +
  labs(x='Segment length (1 unit=100 samples)', y='\u03b1')
```


## Build models
Model the impact of truncated segment length (`trunc_length_bin`) on $h$.  
- `lm.0`: Baseline/intercept only model.  
- `lm.1`: Linear effect of segment length.  
- `lm.2`: Quadratic effect of segment length.  

```{r overll_h_model, echo=TRUE, cache=TRUE}
dfa2 = dfa2 %>% 
  mutate(trunc_length_bin_c = trunc_length_bin -1)
# Baseline model
lm.h.real.0 = lmer(data = dfa2,
            h ~1+
              (1 | longname) , REML = FALSE)

# Linear fixed effect of TruncLength
lm.h.real.1 = lmer(data = dfa2,
            h ~1+ trunc_length_bin_c +
              (1 | longname), REML = FALSE)

# Quad fixed effect of TruncLength
lm.h.real.2 = lmer(data = dfa2, h ~1+ trunc_length_bin_c + I(trunc_length_bin_c^2) +
              (1 | longname), REML = FALSE)

anova(lm.h.real.0, lm.h.real.1, lm.h.real.2)
```

We select the model with the quadratic term.  
```{r}
summary(lm.h.real.2)
```

## Interpret Models
For quadratic model $y = B_0 + B_{1Length} + B_{2Length^2}$:  
$-B_1/B_2$: Location as which the downward effect of the quadratic perfectly cancels out the the upward effect of the linear term.  
-Should be equal to $B_0$



```{r interpret_h_model, echo=TRUE}
b_int=fixef(lm.h.real.2)[[1]]
b_lin=fixef(lm.h.real.2)[[2]]
b_quad=fixef(lm.h.real.2)[[3]]
b_int
b_lin
b_quad
```

```{r}
int_out = c(int_out, b_int)
lin_out = c(lin_out, b_lin)
quad_out = c(quad_out, b_quad)
model_out = c(model_out, 'Mean h, real data')
```


```{r echo = TRUE}
# At what x-value are the positive and negative effects are zero?
zero_slope = (-b_lin/b_quad) # Add 1 bc we made shortest segment 0
zero_slope +1
```

At this point, the quadratic equals $B_0$
```{r echo = TRUE}
b_int + b_lin*zero_slope + b_quad*zero_slope^2
```

- $-B_1/(2*B_2)$ = Point at which the quadratic equation levels off (maximum possible value of the curve)
```{r, echo = TRUE}
level_off = -b_lin/(2*b_quad)
level_off +1 
```


- Might also want to interpret effects in terms of SD(y):
```{r}
# With each 100-sample increase in length, how many SDs does h change by?
b_lin_stand = b_lin/sd(dfa2$h) 
b_quad_stand = b_quad/sd(dfa2$h)
b_lin_stand
b_quad_stand
```


## Plot model results 
```{r plot_model_results_h, cache=TRUE}
# Plot model for full dataset 
dfa2$yhat = predict(lm.h.real.2, newdata=dfa2)

my_labs = unique(dfa2$trunc_length_bin)
my_breaks = unique(dfa2$trunc_length_bin_c)
ggplot(data = dfa2, aes(x=trunc_length_bin_c, y=h)) + 
  geom_point(alpha=0.2) + 
  geom_smooth(aes(y=yhat)) +
  scale_y_continuous(limits = c(-1,2.5)) + 
  scale_x_continuous(breaks=my_breaks,labels = my_labs ) + 
  labs(x = 'Truncated Segment Length \n (Each unit=100 samples)', y= '\u03b1') +
  theme_bw() +
  theme(text = element_text(size=16))
```

## Model results excluding time series with poor model fit  
```{r echo = TRUE}
# Number of truncated segments excluded 
sum(dfa2$r2bin==0)
```

```{r model_h_strict, cache=TRUE}
dfa2_strict = dfa2 %>% dplyr::filter(r2bin == 1)
#dfa2_strict = dfa2[dfa2$r2bin==1,]

# Baseline model
lm.h.real.0b = lmer(data = dfa2_strict,
            h ~1+
              (1 | longname) , REML = FALSE)

# Linear fixed effect of TruncLength
lm.h.real.1b = lmer(data = dfa2_strict,
            h ~1+ trunc_length_bin_c +
              (1 | longname), REML = FALSE)

# Quad fixed effect of TruncLength
lm.h.real.2b = lmer(data = dfa2_strict,
            h ~1+ trunc_length_bin_c + I(trunc_length_bin_c^2) +
              (1 | longname), REML = FALSE)

anova(lm.h.real.0b, lm.h.real.1b, lm.h.real.2b)
```

Model results for quadratic model when excluding cases with poor model fit:  
```{r include = TRUE}
summary(lm.h.real.2b)
```

<b>Interpret model results when excluding models with poor linear fit</b>
```{r}
b_int=fixef(lm.h.real.2b)[[1]]
b_lin=fixef(lm.h.real.2b)[[2]]
b_quad=fixef(lm.h.real.2b)[[3]]
b_int
b_lin
b_quad
```

```{r}
int_out = c(int_out, b_int)
lin_out = c(lin_out, b_lin)
quad_out = c(quad_out, b_quad)
model_out = c(model_out, 'Mean h, real data, clean')
```


```{r echo=TRUE}
zero_slope = (-b_lin/b_quad) # Add 1 bc we made shortest segment 0
zero_slope +1
```

```{r echo = TRUE}
level_off = -b_lin/(2*b_quad)
level_off +1 
```


# Impact on variability in *h* for  real data
```{r runmods_h, cache = TRUE, include=TRUE, echo =TRUE}
# Output thresholded
lm.thresh.out = list()
lm.thresh.out.strict = list()

# output for filtered
#lm.filtered.out = list()
#lm.filtered.strict.out = list()

# Pull variables you need 
temp = dfa2 %>% dplyr::select(longname, r2, h, trunc_length_bin)

thresh = seq(1, 20)  
#thresh = seq(1,10)
for (m in thresh) {
  # Thresholded datasets (Models w/ increasingly restricted datasets)
  modeldat_thresholded = temp %>% filter(trunc_length_bin >= m ) 
  modeldat_thresholded_strict = temp %>% filter(trunc_length_bin >= m, r2 >= 0.9) 
  # Filtered dataset
  #modeldat_filtered = temp %>% filter(trunc_length_bin == m)
  #modeldat_filtered_strict = temp %>% filter(trunc_length_bin == m, r2 >=0.9)
  ######################
  # Models with filtered data 
  #####################
  # Baseline Model
  # lm.filtered = lmer(data=modeldat_filtered, 
  #                    h ~ 1 + (1|longname), REML = FALSE)
  # lm.filtered.out[[m]] = lm.filtered
  # # Baseline Model (strict)
  # lm.filtered.strict = lmer(data=modeldat_filtered,
  #                           h ~ 1 + (1|longname))
  # lm.filtered.strict.out[[m]] = lm.filtered.strict
  ######################
  # Models with thresholded data
  #####################
  # Baseline model
  lm.thresholded = lmer(data = modeldat_thresholded,
                        h ~1+ (1 | longname) , REML = FALSE)
  lm.thresh.out[[m]] = lm.thresholded
  # Baseline model (strict)
  lm.thresholded_strict = lmer(data = modeldat_thresholded_strict,
               h ~1+ (1 | longname) , REML = FALSE)
  lm.thresh.out.strict[[m]] = lm.thresholded_strict
 
}
```

```{r pull_stats_realdat, cache=TRUE}
icc.thresholded.out = c()
icc.thresholded.out.strict = c()
#icc.filtered.out = c()
#icc.filtered.out.strict = c()

n.thresholded.out = c()
n.thresholded.out.strict = c()
#n.filtered.out = c()
#n.filtered.out.strict = c()

ngroup.thresholded.out = c()
ngroup.thresholded.out.strict = c()
#ngroup.filtered.out = c()
#ngroup.filtered.out.strict = c()

#se.filtered.out = c()
#se.filtered.out.strict = c()
for (m in c(1:length(lm.thresh.out))) {
  # calculate icc & save
  icc.thresholded.out = c(icc.thresholded.out, icc(lm.thresh.out[[m]])$ICC_adjusted)
  icc.thresholded.out.strict = c(icc.thresholded.out.strict, icc(lm.thresh.out.strict[[m]])$ICC_adjusted)
  #icc.filtered.out = c(icc.filtered.out, icc(lm.filtered.out[[m]])$ICC_adjusted)
  #icc.filtered.out.strict = c(icc.filtered.out.strict, icc(lm.filtered.strict.out[[m]])$ICC_adjusted)
  
  # Pull n's and save
  n.thresholded.out = c(n.thresholded.out, nobs(lm.thresh.out[[m]]))
  n.thresholded.out.strict = c(n.thresholded.out.strict, nobs(lm.thresh.out.strict[[m]]))
  #n.filtered.out = c(n.filtered.out, nobs(lm.filtered.out[[m]]))
  #n.filtered.out.strict = c(n.filtered.out.strict, nobs(lm.filtered.strict.out[[m]]))
  
  # Pull n distinct RE grouping factors
  ngroup.thresholded.out = c(ngroup.thresholded.out, getME(lm.thresh.out[[m]], 'l_i'))
  ngroup.thresholded.out.strict = c(ngroup.thresholded.out.strict, getME(lm.thresh.out.strict[[m]], 'l_i'))
  #ngroup.filtered.out = c(ngroup.filtered.out, getME(lm.filtered.out[[m]], 'l_i'))
  #ngroup.filtered.out.strict = c(ngroup.filtered.out.strict, getME(lm.filtered.out[[m]], 'l_i'))
  
  #se.filtered.out = c(se.filtered.out, coef(summary(lm.filtered.out[[m]]))[, 'Std. Error'])
  #se.filtered.out.strict = c(se.filtered.out.strict, coef(summary(lm.filtered.strict.out[[m]]))[,'Std. Error'])
  }
```



```{r icc_df_realdat}
m=length(icc.thresholded.out)
# Make data.frames for plotting
icc.thresholded.df=data.frame(icc=icc.thresholded.out, 
                     n=n.thresholded.out,
                     ngroup = ngroup.thresholded.out,
                     thresh=c(1:m),
                     threshc = c(0:(m-1))) 

icc.thresholded.strict.df=data.frame(icc=icc.thresholded.out.strict,
                     n=n.thresholded.out.strict,
                     ngroup = ngroup.thresholded.out.strict,
                    thresh=c(1:m),
                     threshc = c(0:(m-1)))

```


```{r}
# rbind(icc.thresholded.df %>% mutate(d='Full data'),
#       icc.thresholded.strict.df %>% mutate(d = 'Excluding bad series')) %>%
# ggplot(data = ., aes(x=thresh, y = icc, color = d)) + 
#   geom_point() + 
#   scale_x_continuous(breaks = seq(1,20,2), labels = seq(100,2000,200)) +
#   scale_y_continuous(limits = c(0,1)) +
#   labs(title='Thresholded data')
# 
# 
# rbind(icc.filtered.df %>% mutate(d='Full data'),
#       icc.filtered.strict.df %>% mutate(d = 'Excluding bad series')) %>%
# ggplot(data = ., aes(x=thresh, y = icc, color =d)) + 
#   geom_point() + 
#   scale_x_continuous(breaks = seq(1,20,2), labels = seq(100,2000,200)) +
#   scale_y_continuous(limits = c(0,1)) +
#   labs(title='Filtered data')
# 
# 
# # Plot SE
# rbind(icc.filtered.df %>% mutate(d='Full data'),
#       icc.filtered.strict.df %>% mutate(d = 'Excluding bad series')) %>%
# ggplot(data = ., aes(x=thresh, y = se, color =d)) + 
#   geom_point() + 
#   scale_x_continuous(breaks = seq(1,20,2), labels = seq(100,2000,200)) +
#   labs(title='Filtered data')
```


## Build Models
Want to control for different sample size. 

```{r}
library(betareg)
library(lmtest)

icc.thresholded.df$avg_ts_per_group = icc.thresholded.df$n / icc.thresholded.df$ngroup

lm.0 = betareg(icc ~ 1  + avg_ts_per_group, data=icc.thresholded.df)
lm.1 = betareg(icc ~ threshc+avg_ts_per_group, data=icc.thresholded.df)
lm.2 = betareg(icc ~ threshc + I(threshc^2) +avg_ts_per_group, data=icc.thresholded.df)

lrtest(lm.0, lm.1, lm.2)
summary(lm.2)
```



## Interpret ICC model



## Visualize model results
* Idea here is that you want there to be HIGH betwen-group-variance and LOW within-group-variane (the group is the time series).
```{r icc_realdat}
icc.thresholded.df$yhat = predict(lm.2, newdata=icc.thresholded.df)

ggplot(data=icc.thresholded.df, aes(x=thresh, y=icc)) +
 geom_point(aes(size=n, colour= n)) +
geom_smooth(aes(y=yhat), color='black') +
scale_x_continuous(breaks = seq(1,20,2), labels = seq(100,2000,200)) +
   scale_y_continuous(limits = c(0,1)) +
   labs(x='Included truncated segments of length>=N', y = 'ICC') +
   theme_bw() +
  theme(axis.text = element_text(size = 13)) +
   scale_color_viridis_c(direction = -1) +
  guides(size=FALSE) # Only want legend for color

```

Predicted ICCs - useful bc back-transforms.  
```{r}
datatable(icc.thresholded.df %>% select(thresh,yhat))
```


```{r change_icc_realdat, cache=TRUE}
# Plot change in ICC
icc.thresholded.df$dicc = c(diff(icc.thresholded.df$icc), NA)
ggplot(data=icc.thresholded.df, aes(x=thresh, y= dicc)) +
  geom_point() +
  geom_line() + 
  scale_x_continuous(breaks = seq(1,20,2), labels = seq(100,2000,200)) +
  scale_y_continuous(breaks = seq(0,.1,.01), limits = c(0,0.1)) + 
  labs(y = "Change in ICC", x = 'Minimum segment length included') +
  theme_bw() +
  theme(axis.text = element_text(size = 12), text = element_text(size=13)) +
  # First elbow
  geom_segment(y = 0.032394455, yend = 0.032394455, x=0, xend = 4, color ='red', linetype=2) +
  geom_segment(y=-0.5, yend=0.032394455, x=4, xend=4, color='red', linetype=2) +
  # Second elbow
  geom_segment(y = 0.01840, yend = 0.01840, x=0, xend = 8, color ='red', linetype=2) +
  geom_segment(y=-0.5, yend=0.01840, x=8, xend=8, color='red', linetype=2)
```


## Model Results excluding time series with poor model fit

```{r model_real_strict_icc, echo=TRUE}
# 
icc.thresholded.strict.df$avg_ts_per_group = icc.thresholded.strict.df$n / icc.thresholded.strict.df$ngroup

lm.icc.real.0b = betareg(icc ~ 1  + avg_ts_per_group, data=icc.thresholded.strict.df)
lm.icc.real.1b = betareg(icc ~ 1 + threshc + avg_ts_per_group, data=icc.thresholded.strict.df) 
lm.icc.real.2b = betareg(icc ~ 1 + threshc + I(threshc^2) +avg_ts_per_group , data=icc.thresholded.strict.df)
# 
lrtest(lm.icc.real.0b, lm.icc.real.1b, lm.icc.real.2b)
```

```{r}
summary(lm.icc.real.2b)
```


```{r icc_realdat_strict}
icc.thresholded.strict.df$yhat =predict(lm.icc.real.2b, newdata=icc.thresholded.strict.df)

ggplot(data=icc.thresholded.strict.df, aes(x=thresh, y=icc)) +
  geom_point(aes(size=n, colour= n)) +
  geom_smooth(aes(y=yhat), color='black') + 
  scale_x_continuous(breaks = seq(1,20,2), labels = seq(100,2000,200)) +
  scale_y_continuous(limits = c(0,1)) +
  labs(x='Included truncated segments of length>=N', y = 'ICC') +
  theme_bw() +
  theme(axis.text = element_text(size = 13)) +
  scale_color_viridis_c(direction = -1) +
  guides(size=FALSE) # Only want legend for color 
```

```{r}
icc.thresholded.strict.df$dicc = c(diff(icc.thresholded.strict.df$icc), NA)
ggplot(data=icc.thresholded.strict.df, aes(x=thresh, y= dicc)) +
  geom_point() +
  geom_line() + 
  scale_x_continuous(breaks = seq(1,20,2), labels = seq(100,2000,200)) +
  scale_y_continuous(breaks = seq(0,.1,.01), limits = c(0,0.1)) + 
  labs(y = "Change in ICC", x = 'Minimum segment length included', title='Analysis excluding time series with R2<0.9') +
  theme_bw() +
  theme(axis.text = element_text(size = 12), text = element_text(size=13))
```


# Simulated data
## Sanity checks  

Make sure that there were no file reading errors. 
```{r sanitycheck_sim, echo=FALSE, include=TRUE}
unique(simdat$h_warning)
```

```{r}
n_trunc_sim=simdat%>%
  dplyr::select(id, trunc_end) %>%
  group_by(id) %>%
  summarize(orig_len = max(trunc_end)) %>%
  ungroup() %>%
  mutate(n=(orig_len)/100) %>%
  mutate(n_series = (n*2)-1) %>%
  dplyr::select(-c(n))

```

<b>Included time series before excluding</b>  
```{r, echo = TRUE}
sum(n_trunc_sim$n_series) # total # of truncated series 
summary(n_trunc_sim$n) # summary on length of time series
summary(n_trunc_sim$n_series) # summary on included time series 
```


```{r simdat_clean, echo=FALSE}
simdat = simdat %>% 
  dplyr::select(-c(h_warning)) %>%
  # Length of time series
  mutate(trunc_length=trunc_end-trunc_start,
         trunc_length_bin=round(trunc_length, -2), # Rounding to a neg number means rounding to power of 10
         trunc_length_bin=trunc_length_bin/100) %>%  # Easier for figs 
  mutate(trunc_version=ifelse(trunc_start==1, 'beg','end'))
```

## How many segments were made for each time series?  
```{r makewide_sim, cache=TRUE}
#Create wide data
simdat.wide.beg=dcast(simdat %>%
                   filter(trunc_version=='beg') %>%
                         # trunc_length_bin<=15) %>% 
                   dplyr::select(id, h, trunc_length_bin),
                   id ~ trunc_length_bin,
                   value.var='h')
```


```{r barplot_nsegs_sim}
###########################################
# Missing data patterns
###########################################
# Recode as 1=has data, 0=no data 
miss=simdat.wide.beg %>% dplyr::select(-c('id'))
miss[is.na(miss)]=0
miss[miss!=0]=1
mean_dat=round(colMeans(miss)*100,0)

data.frame(pct_ts = mean_dat, ts_length = seq(100,2800,100)) %>%
  ggplot(data=., aes(x=ts_length, y=pct_ts)) + 
  geom_bar(stat = 'identity') + 
  geom_hline(yintercept = 25, color = 'red') +
  scale_x_continuous(breaks=seq(100,2800,300)) + 
  labs(x='Truncated Segment Length', y = 'Proportion of Time Series Represented') +
  theme_bw() +
  theme(text = element_text(size=16))
```

```{r simdat_filter, echo=TRUE}
simdat = simdat %>% filter(trunc_length_bin <= 22)
```

```{r}
summary(simdat$h)
```


How many segments are included in the simulated data? 
```{r}
# Info on time series length of the original data 
n_trunc_sim=simdat %>%
  dplyr::select(id, trunc_end) %>%
  group_by(id) %>%
  summarize(orig_len = max(trunc_end)) %>%
  ungroup() %>%
  mutate(n=(orig_len)/100) %>%
  mutate(n_series = (n*2)-1) %>%
  dplyr::select(-c(n))
```

<b>Included time series AFTER excluding</b>  
```{r, echo = TRUE}
sum(n_trunc_sim$n_series) # total # of truncated series 
summary(n_trunc_sim$n) # summary on length of time series
summary(n_trunc_sim$n_series) # summary on included time series 
```

- The average rounded length of the original times series was `r mean(n_trunc_sim$orig_len)` (min=`r min(n_trunc_sim$orig_len)`, max=`r max(n_trunc_sim$orig_len)`).  
- Each time series of length *N* generated [(*N*/100)*2]-1 truncated time series.  The average number of truncated time series generated per original series was `r as.integer(mean(n_trunc_sim$n_series))` (min=`r as.integer(min(n_trunc_sim$n_series))`, max=`r as.integer(max(n_trunc_sim$n_series))`).  
- Of the original time series, `r as.integer(sum(n_trunc_sim$orig_len>1000))` were longer than 1,000 samples.  
- There were `r as.integer(nrow(simdat))` simulated time series.  

# Impact of *R2* - simulated series
## Visualize % of good model fit time series for each segment length
```{r r2_barplot_simdat}
simdat$r2bin = ifelse(simdat$r2 <= .9,0,1) # 5th percentile cutoff

prop.good = simdat %>%
  group_by(trunc_length_bin) %>%
  summarize(n_series = n(),
            n_good = sum(r2bin==1)) %>%
  ungroup() %>%
  mutate(prop_good = n_good/n_series,
         prop_good_c = prop_good - 0.5)

ggplot(data=prop.good, aes(x=trunc_length_bin, y = prop_good_c)) + 
  geom_col() +
  coord_flip() + 
  scale_x_continuous(breaks = seq(1,22,2),
                     labels = seq(100,2200, 200)) +
  scale_y_continuous(breaks = seq(0.0, 0.5, 0.1),
                     labels = seq(0.5, 1.0, 0.1)) + 
  labs(y= expression(paste('Proportion of time series with ', R^2, ' >= 0.9')), x = 'Length of truncated series') +
  theme_bw() +
  theme(axis.text = element_text(size=14),
        text = element_text(size=16))
```

```{r echo=TRUE}
quantile(simdat$r2, c(.9, .91, .92, .93))
sum(simdat$r2>=0.9) / nrow(simdat)

```


# Impact on average *h* estimates: simulated data  
## Preliminary Visualizations
<u>Does <b>a) the direction of truncating</b> seem to moderate the relationship between segment length and $h$?</u> 

The range of $H$ estimates gest tighter the longer the segment. No clear impact of Trial type (calver, pix, or social) or whether segment was truncated from the beginning or end of the time series. 
```{r length_vs_H_simdat}
# Does truncating from beginning/end matter? 
ggplot(data=simdat, aes(x=trunc_length_bin, y=h, color=trunc_version)) + 
  geom_point(alpha=.2) + 
  theme_bw() + 
  labs(x='Length of time series segment (1 unit=100 samples)', y='\u03b1') + 
  scale_x_continuous(breaks=unique(dfa2$trunc_length_bin)) 
```

Boxplots of $H$ to show interquartile range. 
```{r h_boxplot_simdat}
ggplot(data=simdat, aes(x=trunc_length_bin, y=h)) +
  geom_boxplot(aes(group=trunc_length_bin, alpha=.2)) + 
  scale_x_continuous(breaks=unique(simdat$trunc_length_bin)) +
  scale_y_continuous(limits = c(-1.07, 2.25)) + # Scale to match visualizations for real data 
  theme(legend.position = 'none', panel.background = element_rect(fill='white', color = 'black')) +
  labs(x='Segment length (1 unit=100 samples)', y='\u03b1')
```

## Build models

Model the impact of truncated segment length (`trunc_length_bin`) on $h$.  
- `lm.0`: Baseline/intercept only model.  
- `lm.1`: Linear effect of segment length.  
- `lm.2`: Quadratic effect of segment length.  

```{r overll_h_model_simdat, echo=TRUE, cache=TRUE}
x=2 # update so it runs again
simdat = simdat %>% 
  mutate(trunc_length_bin_c = trunc_length_bin -1)
# Baseline model
lm.h.sim.0 = lmer(data = simdat,
            h ~1+
              (1 | id) , REML = FALSE)

# Linear fixed effect of TruncLength
lm.h.sim.1 = lmer(data = simdat,
            h ~1+ trunc_length_bin_c +
              (1 | id), REML = FALSE)

# Quad fixed effect of TruncLength
lm.h.sim.2 = lmer(data = simdat,
            h ~1+ trunc_length_bin_c + I(trunc_length_bin_c^2) +
              (1 | id), REML = FALSE)

anova(lm.h.sim.0, lm.h.sim.1, lm.h.sim.2)
```

We select the model with the quadratic term.  
```{r}
summary(lm.h.sim.2)
```

##Interpret model results  

```{r interpret_h_model_sim, echo=TRUE}
b_int=fixef(lm.h.sim.2)[[1]]
b_lin=fixef(lm.h.sim.2)[[2]]
b_quad=fixef(lm.h.sim.2)[[3]]

b_int
b_lin
b_quad
```

```{r}
int_out = c(int_out, b_int)
lin_out = c(lin_out, b_lin)
quad_out = c(quad_out, b_quad)
model_out = c(model_out, 'Mean h, sim data')
```

$-B_1/B_2$: Location as which the downward effect of the quadratic perfectly cancels out the the upward effect of the linear term.  
-Should be equal to $B_0$
Point at which negative quadratic cancels out the postiive linear effect:  
```{r echo = TRUE}
zero_slope = (-b_lin/b_quad) 
(-b_lin/b_quad) + 1
```

At this point, the quadratic equals $B_0$
```{r}
b_int + b_lin*zero_slope + b_quad*zero_slope^2
```


Point at which effect levels off:  
```{r echo = TRUE}
level_off = -b_lin/(b_quad*2) 
level_off + 1 
```

Weights standardized by SD(y):  
```{r echo = TRUE}
# With each 100-sample increase in length, how many SDs does h change by?
b_lin_stand = b_lin/sd(simdat$h) 
b_lin_stand

b_quad_stand = b_quad/sd(simdat$h)
b_quad_stand
```


## Plot model results 
```{r plot_model_results_h_sim, cache=TRUE}
# Plot model for full dataset 
simdat$yhat = predict(lm.h.sim.2, newdata=simdat)

my_labs = unique(simdat$trunc_length_bin)
my_breaks = unique(simdat$trunc_length_bin_c)
ggplot(data = simdat, aes(x=trunc_length_bin_c, y=h)) + 
  geom_point(alpha=0.2) + 
  geom_smooth(aes(y=yhat), method='lm') +
  scale_x_continuous(breaks=my_breaks,labels = my_labs ) + 
  scale_y_continuous(limits = c(-1,2.5)) + 
  labs(x = 'Truncated Segment Length \n (Each unit=100 samples)', y= '\u03b1') +
  theme_bw() +
  theme(text = element_text(size=16))



```


## Model results excluding time series with poor model fit
```{r overll_h_model_simdat_clean, echo=TRUE, cache=TRUE}
simdat2 = simdat %>% 
  mutate(trunc_length_bin_c = trunc_length_bin -1) %>%
  filter(r2 >= 0.9)
# Baseline model
lm.h.sim.0b = lmer(data = simdat2,
            h ~1+
              (1 | id) , REML = FALSE)

# Linear fixed effect of TruncLength
lm.h.sim.1b = lmer(data = simdat2,
            h ~1+ trunc_length_bin_c +
              (1 | id), REML = FALSE)

# Quad fixed effect of TruncLength
lm.h.sim.2b = lmer(data = simdat2,
            h ~1+ trunc_length_bin_c + I(trunc_length_bin_c^2) +
              (1 | id), REML = FALSE)

anova(lm.h.sim.0b, lm.h.sim.1b, lm.h.sim.2b)
```

<b>Interpret model results</b>:   
```{r}
summary(lm.h.sim.2b)
```


```{r interpret_h_model_sim_clean, echo=TRUE}
b_int=fixef(lm.h.sim.2b)[[1]]
b_lin=fixef(lm.h.sim.2b)[[2]]
b_quad=fixef(lm.h.sim.2b)[[3]]

b_int
b_lin
b_quad
```

```{r}
int_out = c(int_out, b_int)
lin_out = c(lin_out, b_lin)
quad_out = c(quad_out, b_quad)
model_out = c(model_out, 'Mean h, sim data, clean')
```

$-B_1/B_2$: Location as which the downward effect of the quadratic perfectly cancels out the the upward effect of the linear term.  
-Should be equal to $B_0$
Point at which negative quadratic cancels out the postiive linear effect:  
```{r echo = TRUE}
zero_slope = (-b_lin/b_quad) 
(-b_lin/b_quad) + 1
```

At this point, the quadratic equals $B_0$
```{r}
b_int + b_lin*zero_slope + b_quad*zero_slope^2
```


Point at which effect levels off:  
```{r echo = TRUE}
level_off = -b_lin/(b_quad*2) 
level_off + 1 
```

Weights standardized by SD(y):  
```{r echo = TRUE}
# With each 100-sample increase in length, how many SDs does h change by?
b_lin_stand = b_lin/sd(simdat$h) 
b_lin_stand

b_quad_stand = b_quad/sd(simdat$h)
b_quad_stand
```


# Impact on variability in *H* estimates for simulated data


```{r runmods_h_sim, cache = TRUE, include=TRUE, echo =TRUE}
# Output thresholded
lm.thresh.sim.out = list()
lm.thresh.sim.out.strict = list()

# Pull variables you need 
sim = simdat %>% dplyr::select(id, r2, h, trunc_length_bin)

thresh = seq(1, 20)  
for (m in thresh) {
  # Thresholded datasets (Models w/ increasingly restricted datasets)
  modeldat_thresholded = sim %>% filter(trunc_length_bin >= m ) 
  modeldat_thresholded_strict = sim %>% filter(trunc_length_bin >= m, r2 >= 0.9) 
 
  ######################
  # Models with thresholded data
  #####################
  # Baseline model
  lm.thresholded = lmer(data = modeldat_thresholded,
                        h ~1+ (1 | id) , REML = FALSE)
  lm.thresh.sim.out[[m]] = lm.thresholded
  # Baseline model (strict)
  lm.thresholded_strict = lmer(data = modeldat_thresholded_strict,
              h ~1+ (1 | id) , REML = FALSE)
  lm.thresh.sim.out.strict[[m]] = lm.thresholded_strict
 
}
```

```{r pull_stats_simdat}
icc.thresholded.out = c()
icc.thresholded.out.strict = c()
n.thresholded.out = c()
n.thresholded.out.strict = c()
ngroup.thresholded.out = c()
ngroup.thresholded.out.strict = c()

se.filtered.out = c()
se.filtered.out.strict = c()
for (m in c(1:length(lm.thresh.out))) {
  # calculate icc & save
  icc.thresholded.out = c(icc.thresholded.out, icc(lm.thresh.sim.out[[m]])$ICC_adjusted)
  icc.thresholded.out.strict = c(icc.thresholded.out.strict, icc(lm.thresh.sim.out.strict[[m]])$ICC_adjusted)
  
  # Pull n's and save
  n.thresholded.out = c(n.thresholded.out, nobs(lm.thresh.sim.out[[m]]))
  n.thresholded.out.strict = c(n.thresholded.out.strict, nobs(lm.thresh.sim.out.strict[[m]]))
  
    # Pull n distinct RE grouping factors
  ngroup.thresholded.out = c(ngroup.thresholded.out, getME(lm.thresh.sim.out[[m]], 'l_i'))
  ngroup.thresholded.out.strict = c(ngroup.thresholded.out.strict, getME(lm.thresh.sim.out.strict[[m]], 'l_i'))
  }
```


```{r}
m=length(icc.thresholded.out)
# Make data.frames for plotting
icc.sim.df=data.frame(icc=icc.thresholded.out, 
                     n=n.thresholded.out,
                     ngroup = ngroup.thresholded.out,
                     thresh=c(1:m),
                     threshc=c(0:(m-1)))
                     
icc.sim.strict.df=data.frame(icc=icc.thresholded.out.strict, 
                     n=n.thresholded.out.strict,
                     ngroup = ngroup.thresholded.out.strict,
                     thresh=c(1:m),
                     threshc=c(0:(m-1)))
```


## Build models

```{r icc.h.mod_sim, cache=TRUE}
icc.sim.df$avg_ts_per_group = icc.sim.df$n / icc.sim.df$ngroup

lm.0 = betareg(icc ~ 1  + avg_ts_per_group, data=icc.sim.df)
lm.1 = betareg(icc ~ threshc+avg_ts_per_group, data=icc.sim.df)
lm.2 = betareg(icc ~ threshc + I(threshc^2) +avg_ts_per_group, data=icc.sim.df)

lrtest(lm.0, lm.1, lm.2)
summary(lm.2)
```

## Visualize the impact of segment length in ICC
* Idea here is that you want there to be HIGH betwen-group-variance and LOW within-group-variane (the group is the time series).
```{r icc_simdat_plot}
icc.sim.df$yhat = predict(lm.2, newdata=icc.sim.df)
ggplot(data=icc.sim.df, aes(x=thresh, y=icc)) +
  geom_point(aes(size=n, colour= n)) +
  geom_smooth(aes(y=yhat), color='black') + 
  scale_x_continuous(breaks = seq(1,20,2), labels = seq(100,2000,200)) +
  scale_y_continuous(limits = c(0,1)) + 
  labs(x='Included truncated segments of length>=N', y = 'ICC', title='Simulated Data') +
  theme_bw() +
  scale_color_viridis_c(direction = -1) +
  theme(axis.text = element_text(size = 13)) +
  guides(size=FALSE) # Only want legend for color 

```

Predicted ICCs  
```{r}
datatable(icc.sim.df %>% select(thresh,yhat))
```



```{r dicc_simdat_plot}
icc.sim.df$dicc = c(diff(icc.sim.df$icc), NA)
ggplot(data=icc.sim.df, aes(x=thresh, y= dicc)) +
  geom_point() +
  geom_line() + 
  scale_x_continuous(breaks = seq(1,20,2), labels = seq(100,2000,200)) +
  scale_y_continuous(breaks = seq(0,.1,.01), limits = c(0,0.1)) + 
  labs(y = "Change in ICC", x = 'Minimum segment length included', title = 'Simulated time series') +
  theme_bw() +
  theme(axis.text = element_text(size = 13), text = element_text(size=14)) +
  # First elbow
  geom_segment(y = 0.0406797, yend = 0.0406797, x=0, xend = 5, color ='red', linetype=2) +
  geom_segment(y=-0.5, yend=0.0406797, x=5, xend=5, color='red', linetype=2) +
  # Second elbow
  geom_segment(y = 0.012267, yend = 0.012267, x=0, xend = 9, color ='red', linetype=2) +
  geom_segment(y=-0.5, yend=0.012267, x=9, xend=9, color='red', linetype=2)
```

```{r}
datatable(icc.sim.df %>% select(thresh, dicc))
```


## Model results excluding time series with poor fit 

```{r icc.h.mod_sim_clean}
icc.sim.strict.df$avg_ts_per_group = icc.sim.strict.df$n / icc.sim.strict.df$ngroup

lm.0b = betareg(icc ~ 1  + avg_ts_per_group, data=icc.sim.strict.df)
lm.1b = betareg(icc ~ threshc+avg_ts_per_group, data=icc.sim.strict.df)
lm.2b = betareg(icc ~ threshc + I(threshc^2) +avg_ts_per_group, data=icc.sim.strict.df)

lrtest(lm.0, lm.1, lm.2)
summary(lm.2)

```

```{r}
summary(lm.2)
```

# How much data do we save?
```{r}
dat = read.csv('/Users/sifre002/Documents/Code/fractal-eye-analyses/data/cleaned_data_incl_too_short.csv')
dat = dat %>% 
  dplyr::select(id, movie, seg, longestFixDur, remove_tooshort) %>%
  mutate(longestFix_frames = longestFixDur / 3.33) # Convert ms to frames
```

Of the original sample (14,187), # of time series excluded for being too short 
```{r}
n_too_short = dat %>%
  filter(remove_tooshort == 1) %>%
  nrow()
n_too_short
```

Of the time series excluded for being too short, how many would have been saved?
```{r}
n_saved = dat %>%
  filter(remove_tooshort==1) %>%
  mutate(length_bin = floor(longestFix_frames/100)) %>%
  # Calculate the number of time series of each length that were excluded
  group_by(length_bin) %>%
  summarize(n=n()) %>%
  ungroup() 

# What proportion of excluded time series were that length?
n_saved$prop = n_saved$n/n_too_short

n_saved %>%
  # Re-order length in descending order, so cumsum() generates the numbers you need 
  arrange(-length_bin) %>%
  mutate(cumn = cumsum(n),
         cumprop = cumsum(prop))

```

Proportion of time series we excluded that we could have saved
```{r}
n_saved/n_too_short
```

